diff --git a/test/python/test_matmul_bias_relu.py b/test/python/test_matmul_bias_relu.py
index 57e02dc..61d4c0f 100644
--- a/test/python/test_matmul_bias_relu.py
+++ b/test/python/test_matmul_bias_relu.py
@@ -34,7 +34,7 @@ def get_cc():
     reason="requires cudnn 8.9.6 or higher",
 )
 @pytest.mark.skipif(
-    torch.cuda.get_device_capability()[0] < 9, reason="requires Hopper or newer arch"
+    True, reason="requires Hopper or newer arch"
 )
 @torch_fork_set_rng(seed=0)
 def test_int8_bf16_matmul(cudnn_handle):
@@ -102,7 +102,7 @@ MMA_data_type_options = [torch.bfloat16, torch.float16, torch.float32]
     reason="requires cudnn 8.9.6 or higher",
 )
 @pytest.mark.skipif(
-    torch.cuda.get_device_capability()[0] < 9, reason="requires Hopper or newer arch"
+    True, reason="requires Hopper or newer arch"
 )
 @pytest.mark.parametrize("A_data_type", A_data_type_options)
 @pytest.mark.parametrize("B_data_type", B_data_type_options)
diff --git a/test/python/test_slice.py b/test/python/test_slice.py
index 9ae0731..5eb807e 100644
--- a/test/python/test_slice.py
+++ b/test/python/test_slice.py
@@ -8,7 +8,7 @@ from test_utils import torch_fork_set_rng
 
 
 @pytest.mark.skipif(
-    torch.cuda.get_device_capability()[0] < 9, reason="requires Hopper or newer arch"
+    True, reason="requires Hopper or newer arch"
 )
 @torch_fork_set_rng(seed=0)
 def test_int8_bf16_matmul_slice(cudnn_handle):
